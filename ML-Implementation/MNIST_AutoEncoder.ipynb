{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import the relevant modules\n",
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Import CNTK\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "\n",
    "try:\n",
    "    from urllib.request import urlretrieve\n",
    "except ImportError:\n",
    "    from urllib import urlretrieve\n",
    "    \n",
    "import gzip\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import struct\n",
    "import sys\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refrences\n",
    "\n",
    "https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians\n",
    "\n",
    "https://jaan.io/what-is-variational-autoencoder-vae-tutorial/\n",
    "\n",
    "https://github.com/altosaar/variational-autoencoder/blob/master/vae.py\n",
    "\n",
    "https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory is E:\\Temp\\MNIST\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure the training and test data is generated and available for this tutorial.\n",
    "# We search in two locations in the toolkit for the cached MNIST data set.\n",
    "data_found = False\n",
    "for data_dir in [os.path.join(\"E:\\\\\", \"Temp\", \"MNIST\")]:\n",
    "    train_file = os.path.join(data_dir, \"Train-28x28_cntk_text.txt\")\n",
    "    test_file = os.path.join(data_dir, \"Test-28x28_cntk_text.txt\")\n",
    "    if os.path.isfile(train_file) and os.path.isfile(test_file):\n",
    "        data_found = True\n",
    "        break\n",
    "\n",
    "if not data_found:\n",
    "    raise ValueError(\"Please generate the data by completing CNTK 103 Part A\")\n",
    "print(\"Data directory is {0}\".format(data_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "encoding_dim = 32\n",
    "output_dim = input_dim\n",
    "# training config\n",
    "epoch_size = 30000        # 30000 samples is half the dataset size\n",
    "minibatch_size = 64\n",
    "num_sweeps_to_train_with = 20 if isFast else 100\n",
    "num_samples_per_sweep = 60000\n",
    "    \n",
    "\n",
    "\n",
    "def decoder(input):\n",
    "    intermediate = C.layers.Sequential([C.layers.Dense(input_dim//2, activation = C.relu), \n",
    "                               C.layers.Dense(input_dim, activation=C.sigmoid)])(input)\n",
    "    return intermediate\n",
    "    \n",
    "def encoder(input):\n",
    "    intermediate = C.layers.Sequential([C.layers.Dense(input_dim//2, activation = C.relu), \n",
    "                               C.layers.Dense(encoding_dim, activation=C.relu)])(input)\n",
    "    \n",
    "    mu = C.layers.Dense(encoding_dim, activation=None)(intermediate)\n",
    "    sigma = C.layers.Dense(encoding_dim, activation=C.relu)(intermediate)\n",
    "\n",
    "    \n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = {}\n",
    "def create_network():\n",
    "    \n",
    "    input = C.input_variable(input_dim)\n",
    "    label = C.input_variable(input_dim)\n",
    "    network['input'] = input\n",
    "    network['label'] = label\n",
    "\n",
    "    # Create the model function\n",
    "    mu, sigma = encoder(input/255.0)\n",
    "    network['mu'] = mu\n",
    "    network['sigma'] = sigma\n",
    "    sample = C.random.normal_like(mu, mean=0, scale=1)\n",
    "    sample = mu + sigma * sample       \n",
    "    model = decoder(sample)    \n",
    "    network['decoding'] = model\n",
    "    return network\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_and_test(reader_train, reader_test):\n",
    "\n",
    "    ###############################################\n",
    "    # Training the model\n",
    "    ###############################################\n",
    "\n",
    "    # Instantiate the input and the label variables\n",
    "    network = create_network()    \n",
    "    \n",
    "    # The labels for this network is same as the input MNIST image.\n",
    "    # Note: Inside the model we are scaling the input to 0-1 range\n",
    "    # Hence we rescale the label to the same range\n",
    "    # We show how one can use their custom loss function\n",
    "    # we will use simple reconstruction error - squared diff \n",
    "    label = network['label']\n",
    "    input = network['input']\n",
    "    model = network['decoding']\n",
    "    mu = network['mu']\n",
    "    sigma = network['sigma']\n",
    "    target = network['label']/255.0\n",
    "    loss = C.losses.squared_error(target,model)\n",
    "    \n",
    "    # the above loss doesn't restrict the encoding domain\n",
    "    # in ideal scenario, to generate fake images we want to feed a\n",
    "    # noise to decoder and get real-like images\n",
    "    # inorder to do this, we need encoding domain to be known. Usually we restrict\n",
    "    # the encoding to be N(0,1), we can sample noise from N(0,1) get real-like images\n",
    "    # we want encoder to predict mean and sd for each element and sample from that for decoding\n",
    "    log_stddev = C.log(sigma)\n",
    "    klloss = - 0.5 * (1 + log_stddev - C.square(mu) - C.square(C.exp(log_stddev)))\n",
    "    loss = loss +  klloss\n",
    "\n",
    "    \n",
    "    label_error  = C.classification_error(model, target)\n",
    "\n",
    "    \n",
    "    num_minibatches_to_train = (num_samples_per_sweep * num_sweeps_to_train_with) // minibatch_size\n",
    "\n",
    "\n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    lr_per_sample = [0.00003]\n",
    "    lr_schedule = C.learning_parameter_schedule_per_sample(lr_per_sample, epoch_size)\n",
    "\n",
    "    # Momentum which is applied on every minibatch_size = 64 samples\n",
    "    momentum_schedule = C.momentum_schedule(0.9126265014311797, minibatch_size)\n",
    "\n",
    "    # We use a variant of the Adam optimizer which is known to work well on this dataset\n",
    "    # Feel free to try other optimizers from\n",
    "    # https://www.cntk.ai/pythondocs/cntk.learner.html#module-cntk.learner\n",
    "    learner = C.fsadagrad(model.parameters,\n",
    "                         lr=lr_schedule, momentum=momentum_schedule)\n",
    "\n",
    "    # Instantiate the trainer\n",
    "    progress_printer = C.logging.ProgressPrinter(0)\n",
    "    trainer = C.Trainer(model, (loss, label_error), learner, progress_printer)\n",
    "\n",
    "    # Map the data streams to the input and labels.\n",
    "    # Note: for autoencoders input == label\n",
    "    input_map = {\n",
    "        input  : reader_train.streams.features,\n",
    "        label  : reader_train.streams.features\n",
    "    }\n",
    "\n",
    "    aggregate_metric = 0\n",
    "    for i in range(num_minibatches_to_train):\n",
    "        # Read a mini batch from the training data file\n",
    "        data = reader_train.next_minibatch(minibatch_size, input_map = input_map)\n",
    "\n",
    "        # Run the trainer on and perform model training\n",
    "        trainer.train_minibatch(data)\n",
    "        samples = trainer.previous_minibatch_sample_count\n",
    "        aggregate_metric += trainer.previous_minibatch_evaluation_average * samples\n",
    "\n",
    "    train_error = (aggregate_metric*100.0) / (trainer.total_number_of_samples_seen)\n",
    "    print(\"Average training error: {0:0.2f}%\".format(train_error))\n",
    "\n",
    "    #############################################################################\n",
    "    # Testing the model\n",
    "    # Note: we use a test file reader to read data different from a training data\n",
    "    #############################################################################\n",
    "\n",
    "    # Test data for trained model\n",
    "    test_minibatch_size = 32\n",
    "    num_samples = 10000\n",
    "    num_minibatches_to_test = num_samples / test_minibatch_size\n",
    "    test_result = 0.0\n",
    "\n",
    "    # Test error metric calculation\n",
    "    metric_numer    = 0\n",
    "    metric_denom    = 0\n",
    "\n",
    "    test_input_map = {\n",
    "        input  : reader_test.streams.features,\n",
    "        label  : reader_test.streams.features\n",
    "    }\n",
    "\n",
    "    for i in range(0, int(num_minibatches_to_test)):\n",
    "\n",
    "        # We are loading test data in batches specified by test_minibatch_size\n",
    "        # Each data point in the minibatch is a MNIST digit image of 784 dimensions\n",
    "        # with one pixel per dimension that we will encode / decode with the\n",
    "        # trained model.\n",
    "        data = reader_test.next_minibatch(test_minibatch_size,\n",
    "                                       input_map = test_input_map)\n",
    "\n",
    "        # Specify the mapping of input variables in the model to actual\n",
    "        # minibatch data to be tested with\n",
    "        eval_error = trainer.test_minibatch(data)\n",
    "\n",
    "        # minibatch data to be trained with\n",
    "        metric_numer += np.abs(eval_error * test_minibatch_size)\n",
    "        metric_denom += test_minibatch_size\n",
    "\n",
    "    # Average of evaluation errors of all test minibatches\n",
    "    test_error = (metric_numer*100.0) / (metric_denom)\n",
    "    print(\"Average test error: {0:0.2f}%\".format(test_error))\n",
    "    \n",
    "    return model, train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\serajago.FAREAST\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\cntk\\learners\\__init__.py:340: RuntimeWarning: When providing the schedule as a number, epoch_size is ignored\n",
      "  warnings.warn('When providing the schedule as a number, epoch_size is ignored', RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate per 1 samples: 3e-05\n",
      " 6.41e+03   6.41e+03       0.88       0.88            64\n",
      " 6.38e+03   6.37e+03      0.896      0.903           192\n",
      " 6.36e+03   6.34e+03      0.913      0.927           448\n",
      " 6.26e+03   6.18e+03      0.884      0.858           960\n",
      " 5.56e+03    4.9e+03      0.819      0.757          1984\n",
      " 4.03e+03   2.54e+03      0.664      0.515          4032\n",
      " 3.07e+03   2.13e+03      0.563      0.462          8128\n",
      " 2.51e+03   1.96e+03      0.446      0.331         16320\n",
      " 2.09e+03   1.67e+03      0.322      0.199         32704\n",
      " 1.77e+03   1.45e+03       0.23      0.138         65472\n",
      " 1.54e+03   1.31e+03      0.159     0.0885        131008\n",
      " 1.38e+03   1.23e+03      0.108     0.0567        262080\n",
      " 1.27e+03   1.15e+03     0.0757     0.0434        524224\n",
      " 1.13e+03        998     0.0529     0.0302       1048512\n",
      "Average training error: 4.95%\n",
      "Average test error: 2.23%\n"
     ]
    }
   ],
   "source": [
    "isFast = True\n",
    "# Read a CTF formatted text (as mentioned above) using the CTF deserializer from a file\n",
    "# Read a CTF formatted text (as mentioned above) using the CTF deserializer from a file\n",
    "def create_reader(path, is_training, input_dim, num_label_classes):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "        labels_viz = C.io.StreamDef(field='labels', shape=num_label_classes, is_sparse=False),\n",
    "        features   = C.io.StreamDef(field='features', shape=input_dim, is_sparse=False)\n",
    "    )), randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n",
    "\n",
    "\n",
    "\n",
    "num_label_classes = 10\n",
    "reader_train = create_reader(train_file, True, input_dim, num_label_classes)\n",
    "reader_test = create_reader(test_file, False, input_dim, num_label_classes)\n",
    "model, simple_ae_train_error, simple_ae_test_error = train_and_test(reader_train,\n",
    "                                                                    reader_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read some data to run the eval\n",
    "num_label_classes = 10\n",
    "reader_eval = create_reader(test_file, False, input_dim, num_label_classes)\n",
    "\n",
    "eval_minibatch_size = 50\n",
    "eval_input_map = { input  : reader_eval.streams.features }\n",
    "\n",
    "eval_data = reader_eval.next_minibatch(eval_minibatch_size,\n",
    "                                  input_map = eval_input_map)\n",
    "\n",
    "img_data = eval_data[input].asarray()\n",
    "\n",
    "# Select a random image\n",
    "np.random.seed()\n",
    "idx = np.random.choice(eval_minibatch_size)\n",
    "\n",
    "orig_image = img_data[idx,:,:]\n",
    "decoded_image = model.eval(orig_image)[0]*255\n",
    "\n",
    "# Print image statistics\n",
    "def print_image_stats(img, text):\n",
    "    print(text)\n",
    "    print(\"Max: {0:.2f}, Median: {1:.2f}, Mean: {2:.2f}, Min: {3:.2f}\".format(np.max(img),\n",
    "                                                                              np.median(img),\n",
    "                                                                              np.mean(img),\n",
    "                                                                              np.min(img)))\n",
    "\n",
    "# Print original image\n",
    "print_image_stats(orig_image, \"Original image statistics:\")\n",
    "\n",
    "# Print decoded image\n",
    "print_image_stats(decoded_image, \"Decoded image statistics:\")\n",
    "\n",
    "\n",
    "# Define a helper function to plot a pair of images\n",
    "def plot_image_pair(img1, text1, img2, text2):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(6, 6))\n",
    "\n",
    "    axes[0].imshow(img1, cmap=\"gray\")\n",
    "    axes[0].set_title(text1)\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(img2, cmap=\"gray\")\n",
    "    axes[1].set_title(text2)\n",
    "    axes[1].axis(\"off\")\n",
    "# Plot the original and the decoded image\n",
    "img1 = orig_image.reshape(28,28)\n",
    "text1 = 'Original image'\n",
    "\n",
    "img2 = decoded_image.reshape(28,28)\n",
    "text2 = 'Decoded image'\n",
    "\n",
    "plot_image_pair(img1, text1, img2, text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.66685939,  0.        ,  0.41162193,\n",
       "         0.38101435,  0.7649737 ,  1.30843389,  0.        ,  0.46245933,\n",
       "         0.        ,  0.        ,  1.27721357,  0.33044845,  1.43980563,\n",
       "         0.88219088,  0.        ,  0.        ,  0.87134933,  0.2124386 ,\n",
       "         0.21936195,  0.        ,  0.58183432,  0.53038526,  1.92033005,\n",
       "         0.97121269,  0.28579801,  0.36218014,  1.05822837,  0.38342437,\n",
       "         0.        ,  0.51377368]], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a random image\n",
    "np.random.seed()\n",
    "idx = np.random.choice(eval_minibatch_size)\n",
    "network['sig'].eval(img_data[idx,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter('W', [], [392 x 784]),\n",
       " Parameter('b', [], [784]),\n",
       " Parameter('W', [], [32 x 392]),\n",
       " Parameter('b', [], [392]),\n",
       " Parameter('W', [], [32 x 32]),\n",
       " Parameter('b', [], [32]),\n",
       " Parameter('W', [], [392 x 32]),\n",
       " Parameter('b', [], [32]),\n",
       " Parameter('W', [], [784 x 392]),\n",
       " Parameter('b', [], [392]),\n",
       " Input('Input3344', [#], [784]),\n",
       " Constant('Constant3346', [], []),\n",
       " Parameter('W', [], [32 x 32]),\n",
       " Parameter('b', [], [32]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network['decoder'].inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
