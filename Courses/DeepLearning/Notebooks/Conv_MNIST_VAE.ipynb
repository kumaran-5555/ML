{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import the relevant modules\n",
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Import CNTK\n",
    "import cntk as C\n",
    "\n",
    "\n",
    "import cntk.io.transforms as xforms\n",
    "import cntk.tests.test_utils\n",
    "#C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "\n",
    "try:\n",
    "    from urllib.request import urlretrieve\n",
    "except ImportError:\n",
    "    from urllib import urlretrieve\n",
    "    \n",
    "import gzip\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import struct\n",
    "import sys\n",
    "    \n",
    "import glob\n",
    "import socket\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if socket.gethostname() == 'dsvm':\n",
    "    root = r'/home/kumaran/Data'\n",
    "else:\n",
    "    root = r'E:\\Temp'\n",
    "    \n",
    "data_dir = os.path.join(root, 'lfw')\n",
    "data_dir = os.path.join(root, 'MNIST')\n",
    "# down data from http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz\n",
    "# model dimensions\n",
    "img_h = 28\n",
    "img_w  = 28\n",
    "input_dim = img_h * img_w\n",
    "num_channels = 3\n",
    "num_classes  = 0\n",
    "latent_dim = 15\n",
    "\n",
    "e_kernel_1 = (5,5)\n",
    "e_stride_1 = (2,2)\n",
    "e_filter_1 = 1\n",
    "e_filter_2 =  128\n",
    "\n",
    "isFast = False\n",
    "epoch_size = 30000        # 30000 samples is half the dataset size\n",
    "minibatch_size = 128\n",
    "num_sweeps_to_train_with = 10 if isFast else 1000\n",
    "num_samples_per_sweep = 60000\n",
    "num_minibatches_to_train = (num_samples_per_sweep * num_sweeps_to_train_with) // minibatch_size\n",
    "num_samples_to_test = 10000\n",
    "\n",
    "#map_file = os.path.join(data_dir, 'cntk_image_data.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare CNTK imageReader data\n",
    "'''\n",
    "with open(map_file, 'w') as out:\n",
    "    for f in glob.glob(data_dir + '\\\\*\\\\*.jpg'):\n",
    "        out.write('{}\\t0\\n'.format(f))\n",
    "'''\n",
    "def plot_image(img1, img2):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(6, 6))\n",
    "\n",
    "    axes[0].imshow(img1, cmap=\"gray\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    axes[1].imshow(img2, cmap=\"gray\")    \n",
    "    axes[1].axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reader_mnist(path, is_training):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "        labels = C.io.StreamDef(field='labels', shape=10, is_sparse=False),\n",
    "        input   = C.io.StreamDef(field='features', shape=784, is_sparse=False)\n",
    "    )), randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n",
    "\n",
    "def create_reader_lfw(map_file, train, is_training):\n",
    "    print(\"Reading map file:\", map_file)    \n",
    "\n",
    "    if not os.path.exists(map_file):\n",
    "        raise RuntimeError(\"This tutorials depends 201A tutorials, please run 201A first.\")\n",
    "\n",
    "    # transformation pipeline for the features has jitter/crop only when training\n",
    "    transforms = []\n",
    "    # train uses data augmentation (translation only)\n",
    "    if train and False:\n",
    "        transforms += [\n",
    "            xforms.crop(crop_type='randomside', side_ratio=0.8)\n",
    "        ]\n",
    "    transforms += [\n",
    "        xforms.scale(width=image_width, height=image_height, channels=num_channels, interpolations='linear')        \n",
    "    ]\n",
    "    # deserializer\n",
    "    return c.io.MinibatchSource(c.io.ImageDeserializer(map_file, c.io.StreamDefs(\n",
    "        input = c.io.StreamDef(field='image', transforms=transforms), # first column in map file is referred to as 'image'\n",
    "        labels   = c.io.StreamDef(field='label', shape=num_classes)      # and second as 'label'\n",
    "    )), randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def encoder_cnn(input):    \n",
    "    with C.layers.default_options(init=C.normal(scale=0.02)):\n",
    "\n",
    "        dfc_dim = 1024\n",
    "        df_dim = 64\n",
    "\n",
    "        print('Discriminator convolution input shape', input.shape)\n",
    "        x = C.reshape(input, (1, img_h, img_w))\n",
    "\n",
    "        #h0 = C.layers.Convolution2D(e_kernel_1, e_filter_1, strides=e_stride_1, pad=True, activation=C.relu)(x)        \n",
    "        #print('h0 shape :', h0.shape)\n",
    "                \n",
    "        h1 = C.layers.Convolution2D(e_kernel_1, e_filter_2, strides=e_stride_1, pad=True, activation=C.relu)(x)        \n",
    "        print('h1 shape :', h1.shape)\n",
    "        \n",
    "        \n",
    "        mu = C.layers.Dense(latent_dim, activation=None)(h1)        \n",
    "        print('mu shape :', mu.shape)\n",
    "        \n",
    "        sig = C.layers.Dense(latent_dim, activation=C.relu, init=C.normal(scale=1))(h1)\n",
    "        print('sig shape :', sig.shape)\n",
    "\n",
    "        return mu, sig\n",
    "    \n",
    "def decoder_cnn(z):\n",
    "    with C.layers.default_options(init=C.normal(scale=0.02)):\n",
    "        \n",
    "        \n",
    "        print('Generator input shape: ', z.shape)\n",
    "        s_h2, s_w2 = img_h//2, img_w//2 #Input shape (14,14)\n",
    "        s_h4, s_w4 = img_h//4, img_w//4 # Input shape (7,7)\n",
    "        gfc_dim = 1024\n",
    "        gf_dim = 64\n",
    "        \n",
    "        h1 = C.layers.Dense([e_filter_2, s_h2,  s_w2], activation=C.relu)(z)        \n",
    "        print('h1 shape', h1.shape)\n",
    "        \n",
    "        '''\n",
    "        h2 = C.layers.ConvolutionTranspose2D(e_kernel_1,\n",
    "                                  num_filters=e_filter_1,\n",
    "                                  strides=e_stride_1,\n",
    "                                  pad=True,\n",
    "                                  output_shape=(s_h2, s_w2), activation=C.relu)(h1)        \n",
    "        print('h2 shape', h2.shape)\n",
    "        '''\n",
    "\n",
    "        h3 = C.layers.ConvolutionTranspose2D(e_kernel_1,\n",
    "                                  num_filters=e_filter_1,\n",
    "                                  strides=e_stride_1,\n",
    "                                  pad=True,\n",
    "                                  output_shape=(img_h, img_w),\n",
    "                                  activation=C.sigmoid)(h1)\n",
    "        print('h3 shape :', h3.shape)\n",
    "\n",
    "        return C.reshape(h3, img_h * img_w)\n",
    "\n",
    "def decoder(input):\n",
    "    return decoder_cnn(input)\n",
    "    intermediate = C.layers.Sequential([C.layers.Dense(200, activation = C.relu),\n",
    "                                        C.layers.Dense(200, activation = C.relu),\n",
    "                               C.layers.Dense(input_dim, activation=C.sigmoid)])(input)\n",
    "    return intermediate\n",
    "    \n",
    "def encoder(input):\n",
    "    return encoder_cnn(input)\n",
    "    intermediate = C.layers.Sequential([C.layers.Dense(200, activation = C.relu),\n",
    "                                        C.layers.Dense(200, activation = C.relu),\n",
    "                               C.layers.Dense(latent_dim, activation=C.relu)])(input)\n",
    "    \n",
    "    mu = C.layers.Dense(latent_dim, activation=None)(intermediate)\n",
    "    sigma = C.layers.Dense(latent_dim, activation=C.relu)(intermediate)\n",
    "\n",
    "    \n",
    "    return mu, sigma\n",
    "network = {}\n",
    "\n",
    "\n",
    "def create_network():\n",
    "    input = C.input_variable(input_dim)\n",
    "    label = C.input_variable(input_dim)\n",
    "    network['input'] = input\n",
    "    network['label'] = label\n",
    "\n",
    "    # Create the model function\n",
    "    mu, sig = encoder(input/255.0)\n",
    "    \n",
    "    \n",
    "    z = C.random.normal_like(mu, mean=0, scale=1)\n",
    "    z = mu + z * sig\n",
    "    network['mu'] = mu\n",
    "    network['sig'] = sig\n",
    "    \n",
    "    z_placeholder = C.placeholder()            \n",
    "    output = C.as_block(decoder(z_placeholder), [(z_placeholder, z)], 'Decoder', 'Decoder_1')    \n",
    "    network['output'] = output\n",
    "    target = input/255.0\n",
    "    construction_loss = C.losses.squared_error(target, output)\n",
    "    \n",
    "    \n",
    "    l1 = C.element_times(target, C.log(output))\n",
    "    l2 = C.element_times(1.0-target, C.log(1.0-output))\n",
    "    print(l1.shape)\n",
    "    print(l2.shape)\n",
    "                         \n",
    "    #construction_loss = C.reduce_sum(-1 * C.plus(l1, l2))\n",
    "    #print(construction_loss.shape)\n",
    "    construction_loss = C.reduce_sum(C.binary_cross_entropy(output, target))                                 \n",
    "    \n",
    "    log_stddev = C.log(sig)\n",
    "    kl_loss = - 0.5 * C.reduce_sum((1 + C.log(C.square(sig)) - C.square(mu) - C.square(C.exp(log_stddev))))\n",
    "    loss = 0.5 * construction_loss + 0.5 * kl_loss\n",
    "    \n",
    "    error  = C.classification_error(output, target)\n",
    "    network['loss'] = loss\n",
    "    network['error'] = error\n",
    "    return network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator convolution input shape (784,)\n",
      "h1 shape : (128, 14, 14)\n",
      "mu shape : (15,)\n",
      "sig shape : (15,)\n",
      "Generator input shape:  (-2,)\n",
      "h1 shape (-2,)\n",
      "h3 shape : (-2,)\n",
      "(784,)\n",
      "(784,)\n",
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/cntk/learners/__init__.py:340: RuntimeWarning: When providing the schedule as a number, epoch_size is ignored\n",
      "  warnings.warn('When providing the schedule as a number, epoch_size is ignored', RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate per 1 samples: 3e-05\n",
      "      436        436      0.902      0.902           128\n",
      "      434        433      0.901        0.9           384\n",
      "      431        428       0.88      0.864           896\n",
      "      427        423      0.875      0.871          1920\n",
      "      411        397      0.915      0.952          3968\n",
      "      362        315      0.918      0.922          8064\n",
      "      298        236      0.678      0.441         16256\n",
      "      257        217      0.551      0.425         32640\n",
      "      222        187       0.47       0.39         65408\n",
      "      186        151      0.388      0.306        130944\n",
      "      155        123      0.307      0.226        262016\n",
      "      130        105      0.247      0.187        524160\n",
      "      108       86.9      0.205      0.163       1048448\n",
      "     92.1       75.7      0.161      0.116       2097024\n",
      "       80       67.9      0.122     0.0827       4194176\n",
      "     71.4       62.8     0.0927     0.0637       8388480\n",
      "     63.7         56     0.0671     0.0415      16777088\n",
      "       58       52.3     0.0488     0.0306      33554304\n",
      "Average training error: 3.95%\n",
      "Average test error: 3.16%\n"
     ]
    }
   ],
   "source": [
    "def train(train_file, test_file):\n",
    "    create_network()\n",
    "    \n",
    "    loss = network['loss']\n",
    "    output = network['output']\n",
    "    error = network['error']\n",
    "    input = network['input']\n",
    "    label = network['label']\n",
    "    \n",
    "     # Instantiate the trainer object to drive the model training\n",
    "    lr_per_sample = [0.00003]\n",
    "    lr_schedule = C.learning_parameter_schedule_per_sample(lr_per_sample, epoch_size)\n",
    "    # Momentum which is applied on every minibatch_size = 64 samples\n",
    "    momentum_schedule = C.momentum_schedule(0.9126265014311797, minibatch_size)\n",
    "    # We use a variant of the Adam optimizer which is known to work well on this dataset\n",
    "    # Feel free to try other optimizers from\n",
    "    # https://www.cntk.ai/pythondocs/cntk.learner.html#module-cntk.learner\n",
    "    learner = C.fsadagrad(output.parameters,\n",
    "                         lr=lr_schedule, momentum=momentum_schedule)\n",
    "    \n",
    "    #learner = C.adam(output.parameters, lr=lr_schedule, momentum=momentum_schedule)\n",
    "\n",
    "    # Instantiate the trainer\n",
    "    progress_printer = C.logging.ProgressPrinter(0)\n",
    "    trainer = C.Trainer(output, (loss, error), learner, progress_printer)\n",
    "\n",
    "    # Map the data streams to the input and labels.\n",
    "    # Note: for autoencoders input == label\n",
    "    input_map = {\n",
    "        input  : train_file.streams.input,\n",
    "        label  : train_file.streams.labels\n",
    "    }\n",
    "    \n",
    "    test_input_map = {\n",
    "        input  : test_file.streams.input,\n",
    "        label  : test_file.streams.labels\n",
    "    }\n",
    "    aggregate_metric = 0\n",
    "    for i in range(num_minibatches_to_train):\n",
    "        # Read a mini batch from the training data file\n",
    "        data = train_file.next_minibatch(minibatch_size, input_map = input_map)\n",
    "\n",
    "        # Run the trainer on and perform model training\n",
    "        trainer.train_minibatch(data)\n",
    "        samples = trainer.previous_minibatch_sample_count\n",
    "        aggregate_metric += trainer.previous_minibatch_evaluation_average * samples\n",
    "\n",
    "    train_error = (aggregate_metric*100.0) / (trainer.total_number_of_samples_seen)\n",
    "    print(\"Average training error: {0:0.2f}%\".format(train_error))\n",
    "    \n",
    "    metric_numer = 0.0\n",
    "    metric_denom = 0\n",
    "    while True:        \n",
    "        data = test_file.next_minibatch(minibatch_size,\n",
    "                                       input_map = test_input_map)\n",
    "        n = minibatch_size\n",
    "        eval_error = trainer.test_minibatch(data)\n",
    "        metric_numer += np.abs(eval_error * n)\n",
    "        metric_denom += n\n",
    "        if metric_denom > num_samples_to_test:\n",
    "            break\n",
    "    # Average of evaluation errors of all test minibatches\n",
    "    test_error = (metric_numer*100.0) / (metric_denom)\n",
    "    print(\"Average test error: {0:0.2f}%\".format(test_error))\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "train_file = create_reader_mnist(os.path.join(data_dir, 'Train-28x28_cntk_text.txt'), True)\n",
    "test_file = create_reader_mnist(os.path.join(data_dir, 'Test-28x28_cntk_text.txt'), True)\n",
    "\n",
    "train(train_file, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = create_reader_mnist(os.path.join(data_dir, 'Test-28x28_cntk_text.txt'), False)\n",
    "input_map = {\n",
    "        network['input']  : test.streams.input,\n",
    "        network['label']  : test.streams.labels\n",
    "    }\n",
    "data = test.next_minibatch(minibatch_size, input_map=input_map)\n",
    "output = network['output'].eval(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f12d536e358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed()\n",
    "idx = np.random.choice(minibatch_size)\n",
    "in_img = data[network['input']].data.asarray()[idx,:,:].reshape(img_h, img_w)\n",
    "out_img = output[idx,:].reshape(img_h, img_w)\n",
    "plot_image(in_img, out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = network['output'].find_by_name('Decoder_1')\n",
    "\n",
    "\n",
    "x = d.clone('freeze', {d.arguments[0] : C.input_variable(latent_dim)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.90804592e-03, -1.10823974e-01, -1.69394829e-03,\n",
       "        -8.10657628e-03,  9.41572607e-01, -7.62432635e-01,\n",
       "        -7.29749978e-01,  1.41884945e-02,  1.88793170e+00,\n",
       "        -2.46265833e-03, -8.83262791e-03,  4.95558381e-01,\n",
       "        -1.71442103e+00, -6.23207353e-03,  7.17319641e-03]], dtype=float32)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f12d546fb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed()\n",
    "idx = np.random.choice(minibatch_size)\n",
    "C.cntk_py.set_fixed_random_seed(idx)\n",
    "random = C.random.normal((latent_dim), mean=0, scale=1)\n",
    "plot_image(x.eval(random.eval()).reshape(img_h, img_w) , data[network['input']].data.asarray()[idx,:,:].reshape(img_h, img_w)) \n",
    "\n",
    "network['mu'].eval(data[network['input']].data.asarray()[idx,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 784)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x.eval(C.random.normal((32), mean=0, scale=1).eval()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
